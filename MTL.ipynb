{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Model-** Llama3-8b-Instruct"
      ],
      "metadata": {
        "id": "nso5YAgdOoS2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JU3EA6VSNPYn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=pd.read_csv('train.csv')\n",
        "# test_data=pd.read_csv('test.csv')"
      ],
      "metadata": {
        "id": "0J08RYXuQGdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "id": "1xoL7IMrQRgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Using dataframe train_data: format this data according to Llama3-8b-instruct\n",
        "\n",
        "# Assuming 'train_data' is your pandas DataFrame\n",
        "\n",
        "# Function to format the data for Llama 3-8b-instruct\n",
        "def format_for_llama(row):\n",
        "  \"\"\"Formats a row of the DataFrame for Llama 3-8b-instruct.\"\"\"\n",
        "\n",
        "  # Construct the prompt and completion for Llama\n",
        "  prompt = f\"Translate the following text from {row['lang']} to English:\\n\\n{row['comment_text']}\\n\\nTranslation:\"\n",
        "  completion = f\" {row['translated']}\"\n",
        "\n",
        "  # Return the formatted data\n",
        "  return {\"prompt\": prompt, \"completion\": completion}\n",
        "\n",
        "# Apply the formatting function to each row of the DataFrame\n",
        "formatted_data = train_data.apply(format_for_llama, axis=1).tolist()\n",
        "\n",
        "# Now 'formatted_data' contains a list of dictionaries, each with a \"prompt\" and \"completion\"\n",
        "# suitable for training Llama 3-8b-instruct.\n",
        "\n",
        "# Example of how to access the first formatted data point:\n",
        "# print(formatted_data[0])"
      ],
      "metadata": {
        "id": "AFJQ-FxtQv-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "id": "HpJz9nfNT6bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{\n",
        "  instruction:...\n",
        "  input:...\n",
        "  output:...\n",
        "}```"
      ],
      "metadata": {
        "id": "zaJUcxJFS4cj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "{\n",
        "  \"instructions\":\"You are a LLM Model who will identify the current language of the input and provide translated text to english and original language as input\",\n",
        "  \"input\":\"Este usuario ni siquiera llega al rango de hereje . Por lo tanto debería ser quemado en la barbacoa para purificar su alma y nuestro aparato digestivo mediante su ingestión. Skipe linkin 22px Honor, valor, leltad. 17:48 13 mar 2008 (UTC)\",\n",
        "  \"output\":\"This text is in 'Spanish' and the translated text is 'This user does not even make it to the rank of heretic . It should therefore be burned in the barbecue to purify his soul and to our digestive system through ingestion. Skipe linkin 22px Honor, courage, leltad. 17:48 13 mar 2008 (UTC)'\"\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "KdqbBtpYTcjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import json\n",
        "\n",
        "df = train_data\n",
        "\n",
        "# Drop the 'id' and 'toxic' columns\n",
        "df = df.drop(columns=['id', 'toxic'])\n",
        "\n",
        "# Define the instruction, which remains the same for all iterations\n",
        "instruction_text = (\n",
        "    \"You are a LLM Model who will identify the current language of the input \"\n",
        "    \"and provide translated text to English and original full name of the language as input.\"\n",
        ")\n",
        "\n",
        "# Language code mappings\n",
        "lang_map = {\n",
        "    'es': 'Spanish',\n",
        "    'it': 'Italian',\n",
        "    'tr': 'Turkish'\n",
        "}\n",
        "\n",
        "# Create a list to store each JSON object\n",
        "json_output = []\n",
        "\n",
        "# Iterate through each row in the DataFrame and build the JSON format\n",
        "for _, row in df.iterrows():\n",
        "    # Map the language code to the full language name\n",
        "    lang_full = lang_map.get(row['lang'], row['lang'])  # Default to code if not in lang_map\n",
        "    json_obj = {\n",
        "        \"instructions\": instruction_text,\n",
        "        \"input\": row['comment_text'],\n",
        "        \"output\": f\"This text is in {lang_full} and the translated text is {row['translated']}\"\n",
        "    }\n",
        "    json_output.append(json_obj)\n",
        "\n",
        "# Save the JSON data to a file\n",
        "with open('output.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(json_output, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"JSON file created successfully!\")\n"
      ],
      "metadata": {
        "id": "Nf9OnDKyQum8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GGJsC0npZDzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('output.json', 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)"
      ],
      "metadata": {
        "id": "4bcadg4MdLC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import notebook_login\n",
        "# from transformers import TrainingArguments\n",
        "# from trl import SFTTrainer\n",
        "# from unsloth import FastLanguageModel"
      ],
      "metadata": {
        "id": "p2x7uKWcdMgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json,datasets\n",
        "huggingface_user = \"Alapan\"\n",
        "import os\n",
        "dataset_name = \"MTL-Llama3-8b-Instruct\"\n",
        "\n",
        "class Llama3InstructDataset:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.prompts = []\n",
        "        self.create_prompts()\n",
        "\n",
        "    def create_prompt(self, row):\n",
        "        prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>{row['instructions']}<|eot_id|><|start_header_id|>user<|end_header_id|>{row['input']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>{row['output']}<|eot_id|>\"\"\"\n",
        "        return prompt\n",
        "\n",
        "    def create_prompts(self):\n",
        "        for row in self.data:\n",
        "            prompt = self.create_prompt(row)\n",
        "            self.prompts.append(prompt)\n",
        "\n",
        "    def get_dataset(self):\n",
        "        df = pd.DataFrame({'prompt': self.prompts})\n",
        "        return df\n",
        "\n",
        "def create_dataset_hf(dataset):\n",
        "    dataset.reset_index(drop=True, inplace=True)\n",
        "    # return datasets.DatasetDict({\"train\":Dataset.from_pandas(dataset)})\n",
        "    return DatasetDict({\"train\": Dataset.from_pandas(dataset)})\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    with open('output.json', 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "\n",
        "    dataset = Llama3InstructDataset(data)\n",
        "    df = dataset.get_dataset()\n",
        "\n",
        "    processed_data_path = 'processed_data'\n",
        "    os.makedirs(processed_data_path, exist_ok=True)\n",
        "    llama3_dataset = create_dataset_hf(df)\n",
        "    llama3_dataset.save_to_disk(os.path.join(processed_data_path, \"llama3_dataset\"))\n",
        "    llama3_dataset.push_to_hub(f\"{huggingface_user}/{dataset_name}\")"
      ],
      "metadata": {
        "id": "XfE5cNSzYQ0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"hugging_face_username\":huggingface_user,\n",
        "    \"model_config\": {\n",
        "        \"base_model\":\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # The base model\n",
        "        \"finetuned_model\":\"llama-3-8b-Instruct-bnb-4bit-aiaustin-demo\", # The finetuned model\n",
        "        \"max_seq_length\": 2048, # The maximum sequence length\n",
        "        \"dtype\":torch.float16, # The data type\n",
        "        \"load_in_4bit\": True, # Load the model in 4-bit\n",
        "    },\n",
        "    \"lora_config\": {\n",
        "      \"r\": 16, # The number of LoRA layers 8, 16, 32, 64\n",
        "      \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"], # The target modules\n",
        "      \"lora_alpha\":16, # The alpha value for LoRA\n",
        "      \"lora_dropout\":0, # The dropout value for LoRA\n",
        "      \"bias\":\"none\", # The bias for LoRA\n",
        "      \"use_gradient_checkpointing\":True, # Use gradient checkpointing\n",
        "      \"use_rslora\":False, # Use RSLora\n",
        "      \"use_dora\":False, # Use DoRa\n",
        "      \"loftq_config\":None # The LoFTQ configuration\n",
        "    },\n",
        "    \"training_dataset\":{\n",
        "        \"name\":f\"{huggingface_user}/{dataset_name}\", # The dataset name(huggingface/datasets)\n",
        "        \"split\":\"train\", # The dataset split\n",
        "        \"input_field\":\"prompt\", # The input field\n",
        "    },\n",
        "    \"training_config\": {\n",
        "        \"per_device_train_batch_size\": 2, # The batch size\n",
        "        \"gradient_accumulation_steps\": 4, # The gradient accumulation steps\n",
        "        \"warmup_steps\": 5, # The warmup steps\n",
        "        \"max_steps\":0, # The maximum steps (0 if the epochs are defined)\n",
        "        \"num_train_epochs\": 10, # The number of training epochs(0 if the maximum steps are defined)\n",
        "        \"learning_rate\": 2e-4, # The learning rate\n",
        "        \"fp16\":  torch.cuda.is_bf16_supported(),  # The fp16\n",
        "        \"bf16\": not torch.cuda.is_bf16_supported(), # The bf16\n",
        "        \"logging_steps\": 1, # The logging steps\n",
        "        \"optim\" :\"adamw_8bit\", # The optimizer\n",
        "        \"weight_decay\" : 0.01,  # The weight decay\n",
        "        \"lr_scheduler_type\": \"linear\", # The learning rate scheduler\n",
        "        \"seed\" : 42, # The seed\n",
        "        \"output_dir\" : \"outputs\", # The output directory\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "S4paQgu5nbBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing packages"
      ],
      "metadata": {
        "id": "l8fk_LyOnmC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U xformers --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install --no-deps packaging ninja einops flash-attn trl peft accelerate bitsandbytes\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ],
      "metadata": {
        "id": "qcz-IR6unnoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import notebook_login\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from unsloth import FastLanguageModel"
      ],
      "metadata": {
        "id": "Ekmu0M3nnsGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the model and the tokinizer for the model\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = config.get(\"model_config\").get(\"base_model\"),\n",
        "    max_seq_length = config.get(\"model_config\").get(\"max_seq_length\"),\n",
        "    dtype = config.get(\"model_config\").get(\"dtype\"),\n",
        "    load_in_4bit = config.get(\"model_config\").get(\"load_in_4bit\"),\n",
        ")\n",
        "\n",
        "# Setup for QLoRA/LoRA peft of the base model\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = config.get(\"lora_config\").get(\"r\"),\n",
        "    target_modules = config.get(\"lora_config\").get(\"target_modules\"),\n",
        "    lora_alpha = config.get(\"lora_config\").get(\"lora_alpha\"),\n",
        "    lora_dropout = config.get(\"lora_config\").get(\"lora_dropout\"),\n",
        "    bias = config.get(\"lora_config\").get(\"bias\"),\n",
        "    use_gradient_checkpointing = config.get(\"lora_config\").get(\"use_gradient_checkpointing\"),\n",
        "    random_state = 42,\n",
        "    use_rslora = config.get(\"lora_config\").get(\"use_rslora\"),\n",
        "    use_dora = config.get(\"lora_config\").get(\"use_dora\"),\n",
        "    loftq_config = config.get(\"lora_config\").get(\"loftq_config\"),\n",
        ")\n",
        "\n",
        "# Loading the training dataset\n",
        "dataset_train = load_dataset(config.get(\"training_dataset\").get(\"name\"), split = config.get(\"training_dataset\").get(\"split\"))\n",
        "\n",
        "# Setting up the trainer for the model\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset_train,\n",
        "    dataset_text_field = config.get(\"training_dataset\").get(\"input_field\"),\n",
        "    max_seq_length = config.get(\"model_config\").get(\"max_seq_length\"),\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = config.get(\"training_config\").get(\"per_device_train_batch_size\"),\n",
        "        gradient_accumulation_steps = config.get(\"training_config\").get(\"gradient_accumulation_steps\"),\n",
        "        warmup_steps = config.get(\"training_config\").get(\"warmup_steps\"),\n",
        "        max_steps = config.get(\"training_config\").get(\"max_steps\"),\n",
        "        num_train_epochs= config.get(\"training_config\").get(\"num_train_epochs\"),\n",
        "        learning_rate = config.get(\"training_config\").get(\"learning_rate\"),\n",
        "        fp16 = config.get(\"training_config\").get(\"fp16\"),\n",
        "        bf16 = config.get(\"training_config\").get(\"bf16\"),\n",
        "        logging_steps = config.get(\"training_config\").get(\"logging_steps\"),\n",
        "        optim = config.get(\"training_config\").get(\"optim\"),\n",
        "        weight_decay = config.get(\"training_config\").get(\"weight_decay\"),\n",
        "        lr_scheduler_type = config.get(\"training_config\").get(\"lr_scheduler_type\"),\n",
        "        seed = 42,\n",
        "        output_dir = config.get(\"training_config\").get(\"output_dir\"),\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "sYh8L8PXnhcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "eqdITHc8LDuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"trainer_stats.json\", \"w\") as f:\n",
        "    json.dump(trainer_stats, f, indent=4)"
      ],
      "metadata": {
        "id": "OcPRfxlWLHXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained_gguf(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, quantization_method = \"q4_k_m\")\n",
        "model.push_to_hub_gguf(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, quantization_method = \"q4_k_m\")"
      ],
      "metadata": {
        "id": "VUt8EjbSLIEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the fine-tuned model and the tokenizer for inference\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = config.get(\"model_config\").get(\"finetuned_model\"),\n",
        "        max_seq_length = config.get(\"model_config\").get(\"max_seq_length\"),\n",
        "        dtype = config.get(\"model_config\").get(\"dtype\"),\n",
        "        load_in_4bit = config.get(\"model_config\").get(\"load_in_4bit\"),\n",
        "    )\n",
        "\n",
        "# Using FastLanguageModel for fast inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "system_prompt = f\"You are an AI task automator. You will take a users prompt and use first principle reasoning to break the prompt into tasks that you must accomplish within another chat. RESPOND TO THIS MESSAGE ONLY WITH A PYTHON FORMATTED LIST OF TASKS THAT YOU MUST COMPLETE TO TRUTHFULLY AND INTELLIGENTLY ACCOMPLISH THE USERS REQUEST. ASSUME YOU CAN SEARCH THE WEB, WRITE CODE, RUN CODE, DEBUG CODE, AND AUTOMATE ANYTHING ON THE USERS COMPUTER TO ACCOMPLISH THE PROMPT. CORRECT RESPONSE FORMAT: ['task 1', 'task 2', 'task 3']\"\n",
        "\n",
        "# Tokenizing the input and generating the output\n",
        "prompt = input('TYPE PROMPT TO LLAMA3: ')\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    f\"<|start_header_id|>system<|end_header_id|>{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>{prompt}<|end_header_id|>\"\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True)\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens = True)"
      ],
      "metadata": {
        "id": "HxHCo8zdMybF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}